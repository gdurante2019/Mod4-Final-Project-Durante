{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Mod-4-Project---Starter-Notebook\" data-toc-modified-id=\"Mod-4-Project---Starter-Notebook-1\">Mod 4 Project - Starter Notebook</a></span></li><li><span><a href=\"#Some-Notes-Before-Starting\" data-toc-modified-id=\"Some-Notes-Before-Starting-2\">Some Notes Before Starting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Wide-Format-vs-Long-Format\" data-toc-modified-id=\"Wide-Format-vs-Long-Format-2.1\">Wide Format vs Long Format</a></span></li></ul></li><li><span><a href=\"#Helper-Functions-Provided\" data-toc-modified-id=\"Helper-Functions-Provided-3\">Helper Functions Provided</a></span></li><li><span><a href=\"#Step-1:-Load-the-Data/Filtering-for-Chosen-Zipcodes\" data-toc-modified-id=\"Step-1:-Load-the-Data/Filtering-for-Chosen-Zipcodes-4\">Step 1: Load the Data/Filtering for Chosen Zipcodes</a></span></li><li><span><a href=\"#Step-2:-Data-Preprocessing\" data-toc-modified-id=\"Step-2:-Data-Preprocessing-5\">Step 2: Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Convert-time-data-type-to-datetime-format\" data-toc-modified-id=\"Convert-time-data-type-to-datetime-format-5.1\">Convert time data type to datetime format</a></span></li><li><span><a href=\"#Fix-problem-with-ZIP-codes-beginning-with-'0'\" data-toc-modified-id=\"Fix-problem-with-ZIP-codes-beginning-with-'0'-5.2\">Fix problem with ZIP codes beginning with '0'</a></span></li><li><span><a href=\"#Creating-US-dataframe-(df_melt)-using-melt-function\" data-toc-modified-id=\"Creating-US-dataframe-(df_melt)-using-melt-function-5.3\">Creating US dataframe (df_melt) using melt function</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-new-column,-MetroState,-to-address-duplicate-metro-names-in-different-states-(e.g.,-Aberdeen-in-multiple-states)\" data-toc-modified-id=\"Creating-new-column,-MetroState,-to-address-duplicate-metro-names-in-different-states-(e.g.,-Aberdeen-in-multiple-states)-5.3.1\">Creating new column, MetroState, to address duplicate metro names in different states (e.g., Aberdeen in multiple states)</a></span></li><li><span><a href=\"#Creating-list-of-unique-metro-areas-(metro_state_list)\" data-toc-modified-id=\"Creating-list-of-unique-metro-areas-(metro_state_list)-5.3.2\">Creating list of unique metro areas (metro_state_list)</a></span></li></ul></li><li><span><a href=\"#Calculating-various-numerical-measures-by-zipcodes\" data-toc-modified-id=\"Calculating-various-numerical-measures-by-zipcodes-5.4\">Calculating various numerical measures by zipcodes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-df_melt_means:--means-of-all-values-for-each-zipcode\" data-toc-modified-id=\"Creating-df_melt_means:--means-of-all-values-for-each-zipcode-5.4.1\">Creating df_melt_means:  means of all values for each zipcode</a></span></li><li><span><a href=\"#Creating-df_melt_max-by-finding-max-value-for-each-zipcode\" data-toc-modified-id=\"Creating-df_melt_max-by-finding-max-value-for-each-zipcode-5.4.2\">Creating df_melt_max by finding max value for each zipcode</a></span></li><li><span><a href=\"#Creating-df_melt_mins-(minimum-values-by-zipcode)\" data-toc-modified-id=\"Creating-df_melt_mins-(minimum-values-by-zipcode)-5.4.3\">Creating df_melt_mins (minimum values by zipcode)</a></span></li><li><span><a href=\"#Creating-df_melt_calcs-by-concatenating-dfs-containing-mean,-min,-and-max-columns\" data-toc-modified-id=\"Creating-df_melt_calcs-by-concatenating-dfs-containing-mean,-min,-and-max-columns-5.4.4\">Creating df_melt_calcs by concatenating dfs containing mean, min, and max columns</a></span></li><li><span><a href=\"#Creating-'max_min'-(maximum---minimum)-column\" data-toc-modified-id=\"Creating-'max_min'-(maximum---minimum)-column-5.4.5\">Creating 'max_min' (maximum - minimum) column</a></span></li><li><span><a href=\"#Creating-'max_min_pct'-column-with-percentage-increase-over-minimum-of-max-minus-min\" data-toc-modified-id=\"Creating-'max_min_pct'-column-with-percentage-increase-over-minimum-of-max-minus-min-5.4.6\">Creating 'max_min_pct' column with percentage increase over minimum of max minus min</a></span></li><li><span><a href=\"#Creating-max_mean-(max-minus-mean)-and-mean_min-(mean-minus-min)-columns\" data-toc-modified-id=\"Creating-max_mean-(max-minus-mean)-and-mean_min-(mean-minus-min)-columns-5.4.7\">Creating max_mean (max minus mean) and mean_min (mean minus min) columns</a></span></li><li><span><a href=\"#Creating-'max_mean_pct'-and-'mean_min_pct'-(percentage-increases-of-max-mean-/-mean-and--mean-min-/-min)\" data-toc-modified-id=\"Creating-'max_mean_pct'-and-'mean_min_pct'-(percentage-increases-of-max-mean-/-mean-and--mean-min-/-min)-5.4.8\">Creating 'max_mean_pct' and 'mean_min_pct' (percentage increases of max-mean / mean and  mean-min / min)</a></span></li><li><span><a href=\"#Finding-maxes-and-mins-(and-sometimes-median-values)-for-numerical-columns\" data-toc-modified-id=\"Finding-maxes-and-mins-(and-sometimes-median-values)-for-numerical-columns-5.4.9\">Finding maxes and mins (and sometimes median values) for numerical columns</a></span></li><li><span><a href=\"#Creating-functions-to-find-max,-find-min,-and-find-median\" data-toc-modified-id=\"Creating-functions-to-find-max,-find-min,-and-find-median-5.4.10\">Creating functions to find max, find min, and find median</a></span></li></ul></li><li><span><a href=\"#Creating-df_metro-(US-metro-df)-with-monthly-values-by-Zip\" data-toc-modified-id=\"Creating-df_metro-(US-metro-df)-with-monthly-values-by-Zip-5.5\">Creating df_metro (US metro df) with monthly values by Zip</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-dictionaries-for-iterating-through-dataframes\" data-toc-modified-id=\"Creating-dictionaries-for-iterating-through-dataframes-5.5.1\">Creating dictionaries for iterating through dataframes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-ordict_zips_sizerank-(dictionary-of-zip-codes-and-size-ranks)\" data-toc-modified-id=\"Create-ordict_zips_sizerank-(dictionary-of-zip-codes-and-size-ranks)-5.5.1.1\">Create ordict_zips_sizerank (dictionary of zip codes and size ranks)</a></span></li><li><span><a href=\"#Create-dict_sizerank_zips-(dictionary-of-size-ranks-and-zip-codes--keys:values-reversed-from-previous-dictionary)\" data-toc-modified-id=\"Create-dict_sizerank_zips-(dictionary-of-size-ranks-and-zip-codes--keys:values-reversed-from-previous-dictionary)-5.5.1.2\">Create dict_sizerank_zips (dictionary of size ranks and zip codes--keys:values reversed from previous dictionary)</a></span></li><li><span><a href=\"#Creating-dict_metrostate_zips-(dictionary-with-zips-per-unique-metro-area-(MetroState))\" data-toc-modified-id=\"Creating-dict_metrostate_zips-(dictionary-with-zips-per-unique-metro-area-(MetroState))-5.5.1.3\">Creating dict_metrostate_zips (dictionary with zips per unique metro area (MetroState))</a></span></li><li><span><a href=\"#Creating-ordict_num_metro_zip_counts_sorted-(dictionary-reverse-sorting-of-number-of-zip-codes-in-a-unique-metro-area)\" data-toc-modified-id=\"Creating-ordict_num_metro_zip_counts_sorted-(dictionary-reverse-sorting-of-number-of-zip-codes-in-a-unique-metro-area)-5.5.1.4\">Creating ordict_num_metro_zip_counts_sorted (dictionary reverse-sorting of number of zip codes in a unique <em>metro area</em>)</a></span></li><li><span><a href=\"#Creating-dict_allcities_zips-(zips-for-each-city)\" data-toc-modified-id=\"Creating-dict_allcities_zips-(zips-for-each-city)-5.5.1.5\">Creating dict_allcities_zips (zips for each city)</a></span></li><li><span><a href=\"#Create-ordict_num_city_zips_sorted-(dictionary-reverse-sorting-counts-of-zip-codes-by-city)\" data-toc-modified-id=\"Create-ordict_num_city_zips_sorted-(dictionary-reverse-sorting-counts-of-zip-codes-by-city)-5.5.1.6\">Create ordict_num_city_zips_sorted (dictionary reverse-sorting <em>counts</em> of zip codes by city)</a></span></li><li><span><a href=\"#Creating-dict_metro_size-(dictionary-of-metro-areas-and-number-of-data-points-(proxy-for-size-of-metro-area))\" data-toc-modified-id=\"Creating-dict_metro_size-(dictionary-of-metro-areas-and-number-of-data-points-(proxy-for-size-of-metro-area))-5.5.1.7\">Creating dict_metro_size (dictionary of metro areas and number of data points (proxy for size of metro area))</a></span></li><li><span><a href=\"#Creating-list--(list-of-tuples)-from-dict_metro_size-object\" data-toc-modified-id=\"Creating-list--(list-of-tuples)-from-dict_metro_size-object-5.5.1.8\">Creating list  (list of tuples) from dict_metro_size object</a></span></li><li><span><a href=\"#Creating-list-of-the-thirty-largest-metro-areas\" data-toc-modified-id=\"Creating-list-of-the-thirty-largest-metro-areas-5.5.1.9\">Creating list of the thirty largest metro areas</a></span></li><li><span><a href=\"#Creating-list-from-[0]-index-in-each-tuple\" data-toc-modified-id=\"Creating-list-from-[0]-index-in-each-tuple-5.5.1.10\">Creating list from [0] index in each tuple</a></span></li><li><span><a href=\"#Creating-metro_list-(list-of-metro-areas)\" data-toc-modified-id=\"Creating-metro_list-(list-of-metro-areas)-5.5.1.11\">Creating metro_list (list of metro areas)</a></span></li><li><span><a href=\"#Creating-metro_cities-list\" data-toc-modified-id=\"Creating-metro_cities-list-5.5.1.12\">Creating metro_cities list</a></span></li><li><span><a href=\"#Creating-dict_metro_cities-(dictionary-of-metro-areas-and-the-cities-in-each)\" data-toc-modified-id=\"Creating-dict_metro_cities-(dictionary-of-metro-areas-and-the-cities-in-each)-5.5.1.13\">Creating dict_metro_cities (dictionary of metro areas and the cities in each)</a></span></li><li><span><a href=\"#Creating-dict_sorted_metro_cities-(dictionary-of-metro-areas,-sorted-from-largest-to-smallest,-and-the-cities-in-each)\" data-toc-modified-id=\"Creating-dict_sorted_metro_cities-(dictionary-of-metro-areas,-sorted-from-largest-to-smallest,-and-the-cities-in-each)-5.5.1.14\">Creating dict_sorted_metro_cities (dictionary of metro areas, sorted from largest to smallest, and the cities in each)</a></span></li></ul></li></ul></li><li><span><a href=\"#Creating-df_metro_cities-(US-Metros-df-with-City-mean-values-)\" data-toc-modified-id=\"Creating-df_metro_cities-(US-Metros-df-with-City-mean-values-)-5.6\">Creating df_metro_cities (US Metros df with <em>City</em> mean values )</a></span></li><li><span><a href=\"#Creating-df_metro_values-(US-Metros-df-with-Metro-mean-values)\" data-toc-modified-id=\"Creating-df_metro_values-(US-Metros-df-with-Metro-mean-values)-5.7\">Creating df_metro_values (US Metros df with <em>Metro</em> mean values)</a></span></li><li><span><a href=\"#Creating-df_metro_calcs--means,-mins,-maxes,-etc.-for-each-metro-area\" data-toc-modified-id=\"Creating-df_metro_calcs--means,-mins,-maxes,-etc.-for-each-metro-area-5.8\">Creating df_metro_calcs--means, mins, maxes, etc. for each <em>metro</em> area</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-df_melt_means:--means-for-each-metro-area\" data-toc-modified-id=\"Creating-df_melt_means:--means-for-each-metro-area-5.8.1\">Creating df_melt_means:  means for each <em>metro area</em></a></span></li><li><span><a href=\"#Creating-df_metro_max-by-finding-max-value-for-each-metro-area\" data-toc-modified-id=\"Creating-df_metro_max-by-finding-max-value-for-each-metro-area-5.8.2\">Creating df_metro_max by finding max value for each metro area</a></span></li><li><span><a href=\"#Calculating-df_metro_min-(minimum-value-by-metro-area)\" data-toc-modified-id=\"Calculating-df_metro_min-(minimum-value-by-metro-area)-5.8.3\">Calculating df_metro_min (minimum value by metro area)</a></span></li><li><span><a href=\"#Concatenating-dfs-with-means,-mins,-and-maxes\" data-toc-modified-id=\"Concatenating-dfs-with-means,-mins,-and-maxes-5.8.4\">Concatenating dfs with means, mins, and maxes</a></span></li><li><span><a href=\"#Creating-calculation-columns\" data-toc-modified-id=\"Creating-calculation-columns-5.8.5\">Creating calculation columns</a></span></li></ul></li><li><span><a href=\"#Iterating-over-dictionary-to-plot-values-by-largest-metro-areas\" data-toc-modified-id=\"Iterating-over-dictionary-to-plot-values-by-largest-metro-areas-5.9\">Iterating over dictionary to plot values by largest metro areas</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plotting-top-30-metro-areas\" data-toc-modified-id=\"Plotting-top-30-metro-areas-5.9.1\">Plotting top 30 metro areas</a></span></li><li><span><a href=\"#Plotting-metro-area-by-city\" data-toc-modified-id=\"Plotting-metro-area-by-city-5.9.2\">Plotting metro area by city</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plotting-NY,-NY-metro-by-city\" data-toc-modified-id=\"Plotting-NY,-NY-metro-by-city-5.9.2.1\">Plotting NY, NY metro by city</a></span></li></ul></li></ul></li><li><span><a href=\"#Creating-Sacramento-Metro-dataframes\" data-toc-modified-id=\"Creating-Sacramento-Metro-dataframes-5.10\">Creating Sacramento Metro dataframes</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lists-and-dictionaries\" data-toc-modified-id=\"Lists-and-dictionaries-5.10.1\">Lists and dictionaries</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-sac_metro_cities-list-from-df_sac\" data-toc-modified-id=\"Creating-sac_metro_cities-list-from-df_sac-5.10.1.1\">Creating sac_metro_cities list from df_sac</a></span></li><li><span><a href=\"#Creating-sac_metro_zips-list-from-zips-in-Sac-Metro-area\" data-toc-modified-id=\"Creating-sac_metro_zips-list-from-zips-in-Sac-Metro-area-5.10.1.2\">Creating sac_metro_zips list from zips in Sac Metro area</a></span></li><li><span><a href=\"#Creating-sac_metro_cty-list-from-df_sac\" data-toc-modified-id=\"Creating-sac_metro_cty-list-from-df_sac-5.10.1.3\">Creating sac_metro_cty list from df_sac</a></span></li><li><span><a href=\"#Creating-dict_zips_cities-dictionary-in-an-attempt-to-graph-zips-by-city...\" data-toc-modified-id=\"Creating-dict_zips_cities-dictionary-in-an-attempt-to-graph-zips-by-city...-5.10.1.4\">Creating dict_zips_cities dictionary in an attempt to graph zips by city...</a></span></li></ul></li><li><span><a href=\"#Plots:--Iterating-over-dictionary-to-plot-zip-codes-by-city-or-within-a-city\" data-toc-modified-id=\"Plots:--Iterating-over-dictionary-to-plot-zip-codes-by-city-or-within-a-city-5.10.2\">Plots:  Iterating over dictionary to plot zip codes by city or within a city</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plotting-Sacramento-City-zip-codes-minus-95815,-which-throws-an-error\" data-toc-modified-id=\"Plotting-Sacramento-City-zip-codes-minus-95815,-which-throws-an-error-5.10.2.1\">Plotting Sacramento City zip codes minus 95815, which throws an error</a></span></li><li><span><a href=\"#Plotting-Sacto-Metro-zipcodes-by-city\" data-toc-modified-id=\"Plotting-Sacto-Metro-zipcodes-by-city-5.10.2.2\">Plotting Sacto Metro zipcodes by city</a></span></li></ul></li><li><span><a href=\"#Creating-df_sac_mo_city-(monthly-values-by-City)\" data-toc-modified-id=\"Creating-df_sac_mo_city-(monthly-values-by-City)-5.10.3\">Creating df_sac_mo_city (monthly values by <em>City</em>)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-dataframe-that-shows-max,-min,-dates-for-each,-and-%-difference-max-min-/-min-for-each-city\" data-toc-modified-id=\"Creating-dataframe-that-shows-max,-min,-dates-for-each,-and-%-difference-max-min-/-min-for-each-city-5.10.3.1\">Creating dataframe that shows max, min, dates for each, and % difference max-min / min for each city</a></span></li><li><span><a href=\"#df_sac_cities_calcs\" data-toc-modified-id=\"df_sac_cities_calcs-5.10.3.2\">df_sac_cities_calcs</a></span></li></ul></li><li><span><a href=\"#Plotting-values-by-iterating-through-cities-list\" data-toc-modified-id=\"Plotting-values-by-iterating-through-cities-list-5.10.4\">Plotting values by iterating through cities list</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plotting-boxplots-for-each-city-in-Sacto-Metro-region\" data-toc-modified-id=\"Plotting-boxplots-for-each-city-in-Sacto-Metro-region-5.10.4.1\">Plotting boxplots for each city in Sacto Metro region</a></span></li><li><span><a href=\"#Plotting-boxplots-for-each-zip-in-Sac-Metro,-by-city\" data-toc-modified-id=\"Plotting-boxplots-for-each-zip-in-Sac-Metro,-by-city-5.10.4.2\">Plotting boxplots for each zip in Sac Metro, by city</a></span></li><li><span><a href=\"#Violin-plot-for-Sacramento-city-zip-codes\" data-toc-modified-id=\"Violin-plot-for-Sacramento-city-zip-codes-5.10.4.3\">Violin plot for Sacramento city zip codes</a></span></li><li><span><a href=\"#Line-plot-for-Davis,-CA,-marking-height-of-market-bubble,-drop-of-housing-index-on-national-level,-and-min-for-Davis-post-crash\" data-toc-modified-id=\"Line-plot-for-Davis,-CA,-marking-height-of-market-bubble,-drop-of-housing-index-on-national-level,-and-min-for-Davis-post-crash-5.10.4.4\">Line plot for Davis, CA, marking height of market bubble, drop of housing index on national level, and min for Davis post crash</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Step-3:-EDA-and-Visualization\" data-toc-modified-id=\"Step-3:-EDA-and-Visualization-6\">Step 3: EDA and Visualization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Set-ts-=-zip-code-for-initial-test\" data-toc-modified-id=\"Set-ts-=-zip-code-for-initial-test-6.1\">Set ts = zip code for initial test</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example:--57701\" data-toc-modified-id=\"Example:--57701-6.1.1\">Example:  57701</a></span></li><li><span><a href=\"#Creating-ACF-and-PACF-plots\" data-toc-modified-id=\"Creating-ACF-and-PACF-plots-6.1.2\">Creating ACF and PACF plots</a></span></li><li><span><a href=\"#Seasonal-Decomposition\" data-toc-modified-id=\"Seasonal-Decomposition-6.1.3\">Seasonal Decomposition</a></span></li></ul></li></ul></li><li><span><a href=\"#Step-4:--ARIMA-Modeling\" data-toc-modified-id=\"Step-4:--ARIMA-Modeling-7\">Step 4:  ARIMA Modeling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-p,-d,-q,-and-m-values-for-running-ARIMA-model\" data-toc-modified-id=\"Creating-p,-d,-q,-and-m-values-for-running-ARIMA-model-7.1\">Creating p, d, q, and m values for running ARIMA model</a></span></li><li><span><a href=\"#Setting-up-functions-for-running-ARIMA-models\" data-toc-modified-id=\"Setting-up-functions-for-running-ARIMA-models-7.2\">Setting up functions for running ARIMA models</a></span></li></ul></li><li><span><a href=\"#Step-5:-Interpreting-Results\" data-toc-modified-id=\"Step-5:-Interpreting-Results-8\">Step 5: Interpreting Results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-ARIMA-model-and-show-summary-results-table\" data-toc-modified-id=\"Create-ARIMA-model-and-show-summary-results-table-8.1\">Create ARIMA model and show summary results table</a></span></li><li><span><a href=\"#Create-forecast-model\" data-toc-modified-id=\"Create-forecast-model-8.2\">Create forecast model</a></span></li><li><span><a href=\"#Create-dataframe-to-hold-these-values-and-join-to-existing-dataframe\" data-toc-modified-id=\"Create-dataframe-to-hold-these-values-and-join-to-existing-dataframe-8.3\">Create dataframe to hold these values and join to existing dataframe</a></span></li><li><span><a href=\"#Create-df_new-with-historical-and-forecasted-values\" data-toc-modified-id=\"Create-df_new-with-historical-and-forecasted-values-8.4\">Create df_new with historical and forecasted values</a></span></li><li><span><a href=\"#Plot-forecast-results\" data-toc-modified-id=\"Plot-forecast-results-8.5\">Plot forecast results</a></span></li><li><span><a href=\"#Figure-out-percent-change-in-home-values\" data-toc-modified-id=\"Figure-out-percent-change-in-home-values-8.6\">Figure out percent change in home values</a></span></li><li><span><a href=\"#Compute-and-print-predicted,-best,-and-worst-case-scenarios\" data-toc-modified-id=\"Compute-and-print-predicted,-best,-and-worst-case-scenarios-8.7\">Compute and print predicted, best, and worst case scenarios</a></span></li></ul></li><li><span><a href=\"#QUICK-RUN:--List-of-functions-to-run-for-each-zip-code\" data-toc-modified-id=\"QUICK-RUN:--List-of-functions-to-run-for-each-zip-code-9\">QUICK RUN:  List of functions to run for each zip code</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plot-zip-code(s)-of-interest-over-time,-with-market-highs-and-lows\" data-toc-modified-id=\"Plot-zip-code(s)-of-interest-over-time,-with-market-highs-and-lows-9.1\">Plot zip code(s) of interest over time, with market highs and lows</a></span></li><li><span><a href=\"#Create-ts-dataframe-for-single-zip-code\" data-toc-modified-id=\"Create-ts-dataframe-for-single-zip-code-9.2\">Create ts dataframe for <em>single</em> zip code</a></span><ul class=\"toc-item\"><li><span><a href=\"#Enter-zip-code\" data-toc-modified-id=\"Enter-zip-code-9.2.1\">Enter zip code</a></span></li><li><span><a href=\"#Create-dataframe-from-df_melt-containing-values-for-just-this-zip-code\" data-toc-modified-id=\"Create-dataframe-from-df_melt-containing-values-for-just-this-zip-code-9.2.2\">Create dataframe from df_melt containing values for just this zip code</a></span></li></ul></li><li><span><a href=\"#Plot-ACF-and-PACF-for-ts-dataframe\" data-toc-modified-id=\"Plot-ACF-and-PACF-for-ts-dataframe-9.3\">Plot ACF and PACF for ts dataframe</a></span></li><li><span><a href=\"#Plot-seasonal-decomposition-for-ts-dataframe\" data-toc-modified-id=\"Plot-seasonal-decomposition-for-ts-dataframe-9.4\">Plot seasonal decomposition for ts dataframe</a></span></li><li><span><a href=\"#Generate-list-of-(p,d,q)-and-(p,d,q,m)-values\" data-toc-modified-id=\"Generate-list-of-(p,d,q)-and-(p,d,q,m)-values-9.5\">Generate list of (p,d,q) and (p,d,q,m) values</a></span></li><li><span><a href=\"#Run-ARIMA-model-on-ts-dataframe-with-list-of-p,d,q-values\" data-toc-modified-id=\"Run-ARIMA-model-on-ts-dataframe-with-list-of-p,d,q-values-9.6\">Run ARIMA model on ts dataframe with list of p,d,q values</a></span></li><li><span><a href=\"#General-model-fit-summary\" data-toc-modified-id=\"General-model-fit-summary-9.7\">General model fit summary</a></span></li><li><span><a href=\"#Produce-forecasts-from-ARIMA-model-fit\" data-toc-modified-id=\"Produce-forecasts-from-ARIMA-model-fit-9.8\">Produce forecasts from ARIMA model fit</a></span></li><li><span><a href=\"#Create-dataframe-to-hold-forecast-results\" data-toc-modified-id=\"Create-dataframe-to-hold-forecast-results-9.9\">Create dataframe to hold forecast results</a></span></li><li><span><a href=\"#Create-new-dataframe-concatenating-historic-values-and-forecast-results\" data-toc-modified-id=\"Create-new-dataframe-concatenating-historic-values-and-forecast-results-9.10\">Create new dataframe concatenating historic values and forecast results</a></span></li><li><span><a href=\"#Calculate-predicted,-lower-bound,-and-upper-bound-forecasted-prices\" data-toc-modified-id=\"Calculate-predicted,-lower-bound,-and-upper-bound-forecasted-prices-9.11\">Calculate predicted, lower bound, and upper bound forecasted prices</a></span></li><li><span><a href=\"#Plot-forecast-with-confidence-intervals\" data-toc-modified-id=\"Plot-forecast-with-confidence-intervals-9.12\">Plot forecast with confidence intervals</a></span></li><li><span><a href=\"#Print-summary-of-predicted,-worst-case,-and-best-case-scenarios\" data-toc-modified-id=\"Print-summary-of-predicted,-worst-case,-and-best-case-scenarios-9.13\">Print summary of predicted, worst-case, and best-case scenarios</a></span></li></ul></li><li><span><a href=\"#Ancillary-Functions\" data-toc-modified-id=\"Ancillary-Functions-10\">Ancillary Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setting-time-and-Zip-as-indices-(df_melt2)\" data-toc-modified-id=\"Setting-time-and-Zip-as-indices-(df_melt2)-10.1\">Setting time and Zip as indices (df_melt2)</a></span></li><li><span><a href=\"#Creating-df_cal-from-df_melt2\" data-toc-modified-id=\"Creating-df_cal-from-df_melt2-10.2\">Creating df_cal from df_melt2</a></span></li><li><span><a href=\"#List-of-functions-to-run-for-each-zip-code\" data-toc-modified-id=\"List-of-functions-to-run-for-each-zip-code-10.3\">List of functions to run for each zip code</a></span></li><li><span><a href=\"#Obtain-train-test-split-data\" data-toc-modified-id=\"Obtain-train-test-split-data-10.4\">Obtain train-test-split data</a></span></li><li><span><a href=\"#Define-&quot;get-now&quot;-function\" data-toc-modified-id=\"Define-&quot;get-now&quot;-function-10.5\">Define \"get now\" function</a></span></li><li><span><a href=\"#Import-and-run-SARIMAX-model\" data-toc-modified-id=\"Import-and-run-SARIMAX-model-10.6\">Import and run SARIMAX model</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Mod 4 Project - Starter Notebook\n",
    "\n",
    "This notebook has been provided to you so that you can make use of the following starter code to help with the trickier parts of preprocessing the Zillow dataset. \n",
    "\n",
    "The notebook contains a rough outline the general order you'll likely want to take in this project. You'll notice that most of the areas are left blank. This is so that it's more obvious exactly when you should make use of the starter code provided for preprocessing. \n",
    "\n",
    "**_NOTE:_** The number of empty cells are not meant to infer how much or how little code should be involved in any given step--we've just provided a few for your convenience. Add, delete, and change things around in this notebook as needed!\n",
    "\n",
    "# Some Notes Before Starting\n",
    "\n",
    "This project will be one of the more challenging projects you complete in this program. This is because working with Time Series data is a bit different than working with regular datasets. In order to make this a bit less frustrating and help you understand what you need to do (and when you need to do it), we'll quickly review the dataset formats that you'll encounter in this project. \n",
    "\n",
    "## Wide Format vs Long Format\n",
    "\n",
    "If you take a look at the format of the data in `zillow_data.csv`, you'll notice that the actual Time Series values are stored as separate columns. Here's a sample: \n",
    "\n",
    "<img src='~/../images/df_head.png'>\n",
    "\n",
    "You'll notice that the first seven columns look like any other dataset you're used to working with. However, column 8 refers to the median housing sales values for April 1996, column 9 for May 1996, and so on. This This is called **_Wide Format_**, and it makes the dataframe intuitive and easy to read. However, there are problems with this format when it comes to actually learning from the data, because the data only makes sense if you know the name of the column that the data can be found it. Since column names are metadata, our algorithms will miss out on what dates each value is for. This means that before we pass this data to our ARIMA model, we'll need to reshape our dataset to **_Long Format_**. Reshaped into long format, the dataframe above would now look like:\n",
    "\n",
    "<img src='~/../images/melted1.png'>\n",
    "\n",
    "There are now many more rows in this dataset--one for each unique time and zipcode combination in the data! Once our dataset is in this format, we'll be able to train an ARIMA model on it. The method used to convert from Wide to Long is `pd.melt()`, and it is common to refer to our dataset as 'melted' after the transition to denote that it is in long format. \n",
    "\n",
    "# Helper Functions Provided\n",
    "\n",
    "Melting a dataset can be tricky if you've never done it before, so you'll see that we have provided a sample function, `melt_data()`, to help you with this step below. Also provided is:\n",
    "\n",
    "* `get_datetimes()`, a function to deal with converting the column values for datetimes as a pandas series of datetime objects\n",
    "* Some good parameters for matplotlib to help make your visualizations more readable. \n",
    "\n",
    "Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the Data/Filtering for Chosen Zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fsds_100719.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('zillow_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.RegionName == 95616]     # also acceptable:  df.loc[df['RegionName'] == 95616]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert time data type to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datetimes(df):\n",
    "    return pd.to_datetime(df.columns.values[7:], format='%Y-%m', errors = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_datetimes(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix problem with ZIP codes beginning with '0'\n",
    "\n",
    "I have surmised that RegionName is the ZIP code for each entry.  RegionName values with only 4 digits represent ZIP codes that actually begin with '0'.  So that I can work with ZIP codes in the data set, I will need to add that zero onto every 4-digit RegionName value.  Once that's completed, I'll rename this column \"ZipCode\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df.RegionName.sort_values().head(10)\n",
    "\n",
    "df.sort_values(by=\"RegionName\").head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['RegionName'] = df.RegionName.astype(str)\n",
    "df['RegionName'].dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'RegionName': 'Zip'}, inplace=True)\n",
    "df.columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zips = []\n",
    "\n",
    "for i in df['Zip']:\n",
    "    if len(i) < 5:\n",
    "        i = '0' + i\n",
    "        zips.append(i)\n",
    "    else:\n",
    "        zips.append(i)\n",
    "\n",
    "zips\n",
    "df['Zip'] = pd.Series(zips)\n",
    "df.sort_values(by='Zip').head()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Creating US dataframe (df_melt) using melt function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def melt_data(df):\n",
    "    melted = pd.melt(df, id_vars=['RegionID', 'Zip', 'City', 'State', 'Metro', 'CountyName', 'SizeRank'], var_name='time')\n",
    "    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=False)\n",
    "    melted = melted.dropna(subset=['value'])\n",
    "    return melted   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt = melt_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Dropping RegionID:\n",
    "\n",
    "df_melt.drop('RegionID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Dataframe is sorted by SizeRank by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt['Metro'].fillna('Missing', inplace=True)\n",
    "df_melt.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Sorting by zip code, then time\n",
    "\n",
    "df_melt.sort_values(by=['Zip', 'time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that a few zip codes are missing; these rows were eliminated when Zip codes with NaN values in the 'value' column were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating new column, MetroState, to address duplicate metro names in different states (e.g., Aberdeen in multiple states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt['MetroState'] = df_melt['Metro'] + ' ' + df_melt['State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating list of unique metro areas (metro_state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metro_state_list = list(set(df_melt['MetroState']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(metro_state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(metro_state_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Calculating various numerical measures by zipcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating df_melt_means:  means of all values for each zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_melt_means = round(df_melt.groupby(['Zip', 'City', 'Metro', 'MetroState']).mean().reset_index(),0)\n",
    "df_melt_means.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_means.rename(columns={'value': 'mean'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_melt_means.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating df_melt_max by finding max value for each zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_max = df_melt.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_max = round(df_melt.groupby(['Zip', 'City', 'Metro', 'MetroState']).agg({'value':'max'}).reset_index(), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_max.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_max.rename(columns={'value': 'max'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_max.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating df_melt_mins (minimum values by zipcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_min = round(df_melt.groupby(['Zip', 'City', 'Metro', 'MetroState']).min().reset_index(), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_min.rename(columns={'value': 'min'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt_min.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating df_melt_calcs by concatenating dfs containing mean, min, and max columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs = pd.concat([df_melt_means, df_melt_max['max'], \n",
    "                           df_melt_min['min']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating 'max_min' (maximum - minimum) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['max_min'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['max_min'] = round(df_melt_calcs['max'] - df_melt_calcs['min'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.sort_values(by='SizeRank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.sort_values(by='max_min', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating 'max_min_pct' column with percentage increase over minimum of max minus min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.sort_values(by='mean', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['max_min_pct'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['max_min_pct'] = round((df_melt_calcs['max_min'] / df_melt_calcs['min']) * 100, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating max_mean (max minus mean) and mean_min (mean minus min) columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.sort_values(by='max_min_pct', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['mean_min'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['max_mean'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['max_mean'] = round(df_melt_calcs['max'] - df_melt_calcs['mean'], 0)\n",
    "df_melt_calcs['mean_min'] = round(df_melt_calcs['mean'] - df_melt_calcs['min'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.sort_values(by='mean_min', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating 'max_mean_pct' and 'mean_min_pct' (percentage increases of max-mean / mean and  mean-min / min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.sort_values(by='max_mean', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['mean_min_pct'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['max_mean_pct'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['max_mean_pct'] = round((df_melt_calcs['max_mean'] / df_melt_calcs['mean']) * 100, 0)\n",
    "df_melt_calcs['mean_min_pct'] = round((df_melt_calcs['mean_min'] / df_melt_calcs['min']) * 100, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Finding maxes and mins (and sometimes median values) for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df.loc[df['column_name'] == some_value]\n",
    "\n",
    "# df_melt_calcs['max_ovr_min_%'].max()\n",
    "df_melt_calcs.loc[df_melt_calcs['max_min_pct'] == df_melt_calcs['max_min_pct'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs['mean_min_pct'].max()\n",
    "\n",
    "df_melt_calcs.loc[df_melt_calcs['mean_min_pct'] == df_melt_calcs['mean_min_pct'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating functions to find max, find min, and find median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "calc_cols = df_melt_calcs.columns[6:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# function to find max, min, median of any numeric column\n",
    " \n",
    "def find_max(df, col):\n",
    "    return df.loc[df[col] == df[col].max()]\n",
    "\n",
    "def find_min(df, col):\n",
    "    return df.loc[df[col] == df[col].min()]\n",
    "\n",
    "def find_median(df, col):\n",
    "    return df.loc[df[col] == df[col].median()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in calc_cols:\n",
    "    print(f'Finding max of {col}')\n",
    "    display(find_max(df_melt_calcs, col))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# function to find max, min, median of any numeric column\n",
    " \n",
    "def find_max_simple(df, col):\n",
    "    return f'Finding max of {col}', df['Zip'].loc[df[col] == df[col].max()], df['City'].loc[df[col] == df[col].max()], df['Metro'].loc[df[col] == df[col].max()], df['SizeRank'].loc[df[col] == df[col].max()], df[col].loc[df[col] == df[col].max()]\n",
    "\n",
    "def find_min_simple(df, col):\n",
    "    return f'Finding min of {col}', df['Zip'].loc[df[col] == df[col].min()], df['City'].loc[df[col] == df[col].min()], df['Metro'].loc[df[col] == df[col].min()], df['SizeRank'].loc[df[col] == df[col].min()], df[col].loc[df[col] == df[col].min()]\n",
    "\n",
    "def find_median_simple(df, col):\n",
    "    return f'Finding median of {col}', df['Zip'].loc[df[col] == df[col].median()], df['City'].loc[df[col] == df[col].median()], df['Metro'].loc[df[col] == df[col].median()], df['SizeRank'].loc[df[col] == df[col].median()], df[col].loc[df[col] == df[col].median()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_max_simple(df_melt_calcs, 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_min_simple(df_melt_calcs, 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find_median_simple(df_melt_calcs, 'max_min_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# find_median_simple(df_melt_calcs, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = df_melt_calcs.loc[df_melt_calcs['max'] == df_melt_calcs['max'].max()]\n",
    "print(type(a))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_median(df_melt_calcs, 'max_mean_pct').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_max(df_melt_calcs, 'max_mean_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_min(df_melt_calcs, 'mean_min_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_median(df_melt_calcs, 'max_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_min(df_melt_calcs, 'max_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_min(df_melt_calcs, 'max_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_min(df_melt_calcs, 'mean_min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_min(df_melt_calcs, 'max_min_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_min(df_melt_calcs, 'mean_min_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "find_min(df_melt_calcs, 'max_mean_pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt_calcs.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating df_metro (US metro df) with monthly values by Zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melt.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  the dataframe is sorted by zipcode SizeRank by default.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro = df_melt.groupby(['Metro', 'MetroState', 'CountyName', 'City', 'Zip', 'time']).mean().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metro.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_metro.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dictionaries for iterating through dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create df with just the first date entry for each zip code\n",
    "To create a dictionary, we just need one row with a given zip code; we don't need all of the data values for that zip code; we just need one row with all of the pertinent information related to zip code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_zips_yr1 = df_metro.loc[df_metro['time'] == '1996-12-01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metro_zips_yr1.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_zips_yr1.sort_values(by='SizeRank').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_zips_yr1.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ordict_zips_sizerank (dictionary of zip codes and size ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dict_zips_sizerank = defaultdict(list)\n",
    "\n",
    "for idx,row in df_metro_zips_yr1.iterrows():\n",
    "    dict_zips_sizerank[row['Zip']].append(row['SizeRank'])\n",
    "    \n",
    "print(type(dict_zips_sizerank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to print first n number of items in a dictionary\n",
    "\n",
    "def print_first_n(dictionary, n):\n",
    "    return {k: dictionary[k] for k in list(dictionary)[:n]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_first_n(dict_zips_sizerank, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Create Ordered dictionary\n",
    "ordict_zips_sizerank = OrderedDict(sorted(dict_zips_sizerank.items(), key=lambda item: item[1], reverse=False))\n",
    "type(ordict_zips_sizerank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get subset of dictionary (first 20 key:value pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_first_n(ordict_zips_sizerank, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dict_sizerank_zips (dictionary of size ranks and zip codes--keys:values reversed from previous dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dict_sizerank_zips = defaultdict(list)\n",
    "\n",
    "for idx,row in df_metro_zips_yr1.iterrows():\n",
    "    dict_sizerank_zips[row['SizeRank']].append(row['Zip'])\n",
    "    \n",
    "print(type(dict_sizerank_zips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_first_n(dict_sizerank_zips, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dict_sizerank_zips.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dict_metrostate_zips (dictionary with zips per unique metro area (MetroState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dict_metrostate_zips = defaultdict(list)\n",
    "\n",
    "for idx,row in df_metro_zips_yr1.iterrows():\n",
    "    dict_metrostate_zips[row['MetroState']].append(row['Zip'])\n",
    "    \n",
    "print(type(dict_metrostate_zips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_first_n(dict_metrostate_zips, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in dict_metrostate_zips:\n",
    "#     print(f\"{key}: {dict_metrostate_zips[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_metrostate_zips['New York NY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_metrostate_zips['New York NJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_metrostate_zips['New York PA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating ordict_num_metro_zip_counts_sorted (dictionary reverse-sorting of number of zip codes in a unique *metro area*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_num_metro_zip_counts = {key: len(dict_metrostate_zips[key]) for key in dict_metrostate_zips.keys()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_first_n(dict_num_metro_zip_counts, n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ordered dictionary\n",
    "ordict_metro_zips_sizerank = OrderedDict(sorted(dict_num_metro_zip_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "type(ordict_metro_zips_sizerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(ordict_metro_zips_sizerank)  # prints entire dictionary of metro areas ranked by number of zip codes (largest # first)\n",
    "\n",
    "{k: ordict_metro_zips_sizerank[k] for k in list(ordict_metro_zips_sizerank)[:10]}  # prints top 10 metro areas by number of zip codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dict_allcities_zips (zips for each city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dict_allcities_zips = defaultdict(list)\n",
    "\n",
    "for idx,row in df_metro_zips_yr1.iterrows():\n",
    "    dict_allcities_zips[row['City']].append(row['Zip'])\n",
    "    \n",
    "print(type(dict_allcities_zips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_first_n(dict_allcities_zips, n=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_allcities_zips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dict_allcities_zips['New York'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(dict_allcities_zips.items())[:20]  # same as above, but sorted alphabetically by city name (showing first 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Create ORDERED dictionary for zips in each city, so that we don't have to use the sort function and end up with a tuple\n",
    "\n",
    "ordict_allcities_zips = OrderedDict(sorted(dict_allcities_zips.items(), reverse=False))\n",
    "type(ordict_allcities_zips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_first_n(dict_allcities_zips, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_first_n(ordict_allcities_zips, n=10)   # reminder of what is contained in function:  {k: ordict_allcities_zips[k] for k in list(ordict_allcities_zips)[:n]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create ordict_num_city_zips_sorted (dictionary reverse-sorting *counts* of zip codes by city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_num_city_zips = {key: len(dict_allcities_zips[key]) for key in dict_allcities_zips.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(dict_num_city_zips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_num_city_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ordered dictionary\n",
    "ordict_city_zips_sorted = OrderedDict(sorted(dict_num_city_zips.items(), key=lambda item: item[1], reverse=True))\n",
    "type(ordict_city_zips_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_first_n(ordict_city_zips_sorted, n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dict_metro_size (dictionary of metro areas and number of data points (proxy for size of metro area))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_metro_size = dict(df_metro.MetroState.value_counts())\n",
    "# dict_metro_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(dict_metro_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_metro_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating dictionary object of dict_metro_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_obj = dict_metro_size.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list  (list of tuples) from dict_metro_size object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_obj_list = list(dict_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_obj_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_obj_list[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of the thirty largest metro areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_tup_large_metros_30 = dict_obj_list[:30]\n",
    "list_tup_large_metros_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_tup_large_metros_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = map(lambda x: x[0], tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tup_large_metros_50 = dict_obj_list[:50]\n",
    "len(list_tup_large_metros_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tup_large_metros_50[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tup_large_metros_50                # in order of size\n",
    "b = map(lambda x: x[0], list_tup_large_metros_50)\n",
    "large_metro_list_50 = list(b)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(large_metro_list_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_metro_list_50[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list from [0] index in each tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_tup_large_metros_30\n",
    "b = map(lambda x: x[0], list_tup_large_metros_30)\n",
    "large_metro_list_30 = list(b)\n",
    "large_metro_list_30            # in order of size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating metro_list (list of metro areas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metro_list = list(set(df_metro_cities.MetroState))\n",
    "# metro_list.sort()\n",
    "# metro_list[300:400]\n",
    "metro_list.sort()\n",
    "metro_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating metro_cities list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metro_cities = list(set(df_metro_cities.City))\n",
    "metro_cities.sort()\n",
    "metro_cities[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dict_metro_cities (dictionary of metro areas and the cities in each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_yr_one = df_metro_cities.loc[df_metro_cities['time']=='1996-12-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_yr_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dict_metro_cities = defaultdict(list)\n",
    "\n",
    "for idx,row in df_metro_yr_one.iterrows():\n",
    "    dict_metro_cities[row['MetroState']].append(row['City'])\n",
    "    \n",
    "print(type(dict_metro_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_first_n(dict_metro_cities, n=3)  # print(dict_metro_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_metro_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_metro_cities['New York NY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_metro_cities['New York NJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_metro_values[df_metro_values['MetroState'].str.contains('New York', regex=False)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dict_sorted_metro_cities (dictionary of metro areas, sorted from largest to smallest, and the cities in each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ORDERED cities-by-metro dictionary, ordered by size of metro, sorting by another dictionary!\n",
    "\n",
    "dict_sorted_metros_cities = OrderedDict([(x, dict_metro_cities[x]) for x in dict_metro_size])\n",
    "print_first_n(dict_sorted_metros_cities, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_sorted_metros_cities['New York NY'][:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating df_metro_cities (US Metros df with *City* mean values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_cities = df_metro.groupby(['MetroState', 'CountyName', 'City', 'time']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_cities.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_cities.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_cities.MetroState.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_metro_cities.MetroState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_cities.MetroState.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating df_metro_values (US Metros df with *Metro* mean values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_values = df_melt.groupby(['MetroState', 'State', 'time']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_metro_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_values = df_metro_values.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_values.MetroState.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = df[df['Character Name'] == 'Spellman'].index\n",
    "# df.drop(index, inplace=True)\n",
    "\n",
    "# index = df_metro_values[df_metro_values['MetroState'] == 'Missing'].index\n",
    "# df_metro_values.drop(index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_values.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_values.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metro_values.loc[df_metro_values['MetroState'] == 'New York NY'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_metro_values.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating df_metro_calcs--means, mins, maxes, etc. for each *metro* area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating df_melt_means:  means for each *metro area*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metro_means = round(df_metro_values.groupby(['MetroState']).mean().reset_index(), 0)\n",
    "df_metro_means.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gapminder.rename(columns={'pop':'population',\n",
    "#                           'lifeExp':'life_exp',\n",
    "#                           'gdpPercap':'gdp_per_cap'}, \n",
    "#                  inplace=True)\n",
    "\n",
    "df_metro_means.rename(columns={'value': 'mean'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_metro_means.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating df_metro_max by finding max value for each metro area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_max = round(df_metro_values.groupby(['MetroState']).max().reset_index(), 0)\n",
    "df_metro_max.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_max.rename(columns={'value': 'max'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_max.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating df_metro_min (minimum value by metro area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_min = round(df_metro_values.groupby(['MetroState']).min().reset_index(), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_min.rename(columns={'value': 'min'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metro_min.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating dfs with means, mins, and maxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metro_calcs = pd.concat([df_metro_means, df_metro_max['max'], \n",
    "                           df_metro_min['min']], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_metro_calcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating calculation columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.insert(5,\"max_min\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.insert(6,\"max_mean\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.insert(7,\"mean_min\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.insert(8,\"max_min_pct\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.insert(9,\"max_mean_pct\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.insert(10,\"mean_min_pct\", np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs['max_min'] = round(df_metro_calcs['max'] - df_metro_calcs['min'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs['max_mean'] = round(df_metro_calcs['max'] - df_metro_calcs['mean'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs['mean_min'] = round(df_metro_calcs['mean'] - df_metro_calcs['min'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs['max_min_pct'] = round((df_metro_calcs['max_min'] / df_metro_calcs['min'] * 100), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs['max_mean_pct'] = round((df_metro_calcs['max_mean'] / df_metro_calcs['mean'] * 100), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs['mean_min_pct'] = round((df_metro_calcs['mean_min'] / df_metro_calcs['min'] * 100), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metro_calcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over dictionary to plot values by largest metro areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the code for graphing over the dictionary\n",
    "\n",
    "# for key in sorted(dict_zips_cities.keys()):\n",
    "#     fig, ax = plt.subplots(figsize=(8, 4))\n",
    "#     plt.title(f\"{key}\")\n",
    "#     for val in dict_zips_cities[key]:\n",
    "#         df_sac[df_sac['Zip'] == val].plot(ax = ax)\n",
    "#         ax.legend(dict_zips_cities[key], ncol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting top 30 metro areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adapted from James Irving's study group:   # Standardized y-axis\n",
    "    \n",
    "def plot_ts_metros(df, metros, col='value'):\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 80))\n",
    "    \n",
    "    for i, met in enumerate(metros, start=1):\n",
    "        ax = fig.add_subplot(15,2,i)\n",
    "        \n",
    "        ts = df[col].loc[df['MetroState'] == met]\n",
    "        ts.plot(title = str(met), ax=ax)\n",
    "\n",
    "        max_ = ts.loc['2004':'2011'].idxmax()  \n",
    "        crash = '01-2009'\n",
    "        min_ = ts.loc[crash:].idxmin()\n",
    "\n",
    "        max_all = ts.idxmax()\n",
    "        min_all = ts.idxmin()\n",
    "\n",
    "        ax.axvline(max_, label=f'Max Price: {max_}', color = 'orange', ls=':')\n",
    "        ax.axvline(crash, label = 'Housing Index Drops', color='black')\n",
    "        ax.axvline(min_, label=f'Min Price Post Crash: {min_}', color = 'red', ls=':')\n",
    "        ax.axvline(max_all, label=f'All time series max {max_all}', color = 'green', ls=':')\n",
    "        ax.axvline(min_all, label=f'All time series min: {min_all}', color = 'red', ls=':')\n",
    "        ax.set_ylim(top=1000000)\n",
    "        \n",
    "        ax.legend()\n",
    "    \n",
    "        fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting top 30 metros with standardized y-axis\n",
    "\n",
    "fig, ax = plot_ts_metros(df_metro_values, large_metro_list_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in sorted(dict_sac_zips_cities.keys()):\n",
    "#     fig, ax = plt.subplots(figsize=(12, 4))\n",
    "#     plt.title(f\"{key}\")\n",
    "#     for val in dict_sac_zips_cities[key]:\n",
    "#         df_sac['value'].loc[df_sac['Zip'] == val].plot(ax = ax)\n",
    "#         ax.legend(dict_sac_zips_cities[key], ncol=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from James Irving's study group:   # No standardized y-axis\n",
    "    \n",
    "def plot_ts_metros(df, metros, col='value'):\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 80))\n",
    "    \n",
    "    for i, met in enumerate(metros, start=1):\n",
    "        ax = fig.add_subplot(15,2,i)\n",
    "        \n",
    "        ts = df[col].loc[df['MetroState'] == met]\n",
    "        ts.plot(title = str(met), ax=ax)\n",
    "\n",
    "        max_ = ts.loc['2004':'2011'].idxmax()  \n",
    "        crash = '01-2009'\n",
    "        min_ = ts.loc[crash:].idxmin()\n",
    "\n",
    "        max_all = ts.idxmax()\n",
    "        min_all = ts.idxmin()\n",
    "\n",
    "        ax.axvline(max_, label=f'Max Price: {max_}', color = 'orange', ls=':')\n",
    "        ax.axvline(crash, label = 'Housing Index Drops', color='black')\n",
    "        ax.axvline(min_, label=f'Min Price Post Crash: {min_}', color = 'red', ls=':')\n",
    "        ax.axvline(max_all, label=f'All time series max {max_all}', color = 'green', ls=':')\n",
    "        ax.axvline(min_all, label=f'All time series min: {min_all}', color = 'red', ls=':')\n",
    "\n",
    "        ax.legend()\n",
    "    \n",
    "        fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting top 30 metros, no standardized y-axis\n",
    "\n",
    "fig, ax = plot_ts_metros(df_metro_values, large_metro_list_30)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting metro area by city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ts_metro_cities(df, cities_list, col='value'):\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 80))\n",
    "    \n",
    "    for i, city in enumerate(metro_cities, start=1):\n",
    "        ax = fig.add_subplot(40,3,i)\n",
    "        \n",
    "        ts = df[col].loc[df['City'] == city]\n",
    "        ts.plot(title = str(city), ax=ax)\n",
    "\n",
    "        max_ = ts.loc['2004':'2011'].idxmax()  \n",
    "        crash = '01-2009'\n",
    "        min_ = ts.loc[crash:].idxmin()\n",
    "\n",
    "        max_all = ts.idxmax()\n",
    "        min_all = ts.idxmin()\n",
    "\n",
    "        ax.axvline(max_, label=f'Max Price: {max_}', color = 'orange', ls=':')\n",
    "        ax.axvline(crash, label = 'Housing Index Drops', color='black')\n",
    "        ax.axvline(min_, label=f'Min Price Post Crash: {min_}', color = 'red', ls=':')\n",
    "        ax.axvline(max_all, label=f'All time series max {max_all}', color = 'green', ls=':')\n",
    "        ax.axvline(min_all, label=f'All time series min: {min_all}', color = 'red', ls=':')\n",
    "\n",
    "        ax.legend()\n",
    "    \n",
    "        fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting NY, NY metro by city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting NY, NY metro by city\n",
    "\n",
    "df_metro_cities_nyny = df_metro_cities.loc[df_metro_cities['MetroState'] == 'New York NY']\n",
    "df_metro_cities_nyny.set_index('time', inplace=True)\n",
    "df_metro_cities_nyny.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_cities_nyny = dict_sorted_metros_cities['New York NY']\n",
    "metro_cities_nyny[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in metro_cities_nyny:\n",
    "    col = 'value'\n",
    "    print(df_metro_cities_nyny[col].loc[df_metro_cities_nyny['City'] == city])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ts_metro_cities(df_metro_cities_nyny, metro_cities_nyny, col='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sacramento Metro dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sac = df_melt.loc[df_melt.Metro == 'Sacramento']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Lists and dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Creating sac_metro_cities list from df_sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sac_metro_cities = list(set(df_sac.City))\n",
    "sac_metro_cities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sac_metro_cities.sort()\n",
    "sac_metro_cities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sac['value'].loc[df_sac['City'] == 'Applegate'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Creating sac_metro_zips list from zips in Sac Metro area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sac_metro_zips = list(set(df_sac.Zip))\n",
    "sac_metro_zips[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Creating sac_metro_cty list from df_sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sac_metro_cty = list(set(df_sac.CountyName))\n",
    "sac_metro_cty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Creating dict_zips_cities dictionary in an attempt to graph zips by city..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_mo_one = df_sac.groupby(['CountyName', 'City', 'Zip']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_sac_mo_one.sort_values(by=['Zip']).head(30)\n",
    "\n",
    "#### Seeking to create the dictionary showing just the unique zip code instance rather than repeating it for each row of data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "dict_sac_zips_cities = defaultdict(list)\n",
    "\n",
    "for idx,row in df_sac_mo_one.iterrows():\n",
    "    dict_sac_zips_cities[row['City']].append(row['Zip'])\n",
    "    \n",
    "print(dict_sac_zips_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for key in sorted(dict_sac_zips_cities):\n",
    "    print(f\"{key}: {dict_sac_zips_cities[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots:  Iterating over dictionary to plot zip codes by city or within a city\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the for loop to iterate over dictionary\n",
    "\n",
    "# for key in dict_zips_cities.keys():\n",
    "#     for val in dict_zips_cities[key]:\n",
    "#         print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sac.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Sacramento City zip codes minus 95815, which throws an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sacramento_city_zips = df_sac.loc[df_sac['City'] == 'Sacramento']\n",
    "df_sacramento_city_zips.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sacramento_city_zips['Zip'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sacto_zip_95815 = df_sacramento_city_zips.loc[df_sacramento_city_zips['Zip'] == '95815']\n",
    "df_sacto_zip_95815.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sacto_city_zips_no95815 = df_sacramento_city_zips.drop(df_sacramento_city_zips.loc[df_sacramento_city_zips['Zip'] == '95815'].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sacto_city_zips_no95815"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sacto_zips_no95815 = list(set(df_sacto_city_zips_no95815['Zip']))\n",
    "len(sacto_zips_no95815)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adapted from James Irving's study group:\n",
    "    \n",
    "def plot_ts_zips(df, zipcodes, col='value'):\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 30))\n",
    "    \n",
    "    for i, zc in enumerate(zipcodes, start=1):\n",
    "        ax = fig.add_subplot(7, 3, i)\n",
    "        \n",
    "        ts = df[col].loc[df['Zip'] == zc]\n",
    "        ts.plot(title = zc, ax=ax)\n",
    "\n",
    "        max_ = ts.loc['2004':'2011'].idxmax()  \n",
    "        crash = '01-2009'\n",
    "        min_ = ts.loc[crash:].idxmin()\n",
    "\n",
    "        max_all = ts.idxmax()\n",
    "        min_all = ts.idxmin()\n",
    "\n",
    "        ax.axvline(max_, label=f'Max Price: {max_}', color = 'orange', ls=':')\n",
    "        ax.axvline(crash, label = 'Housing Index Drops', color='black')\n",
    "        ax.axvline(min_, label=f'Min Price Post Crash: {min_}', color = 'red', ls=':')\n",
    "        ax.axvline(max_all, label=f'All time series max {max_all}', color = 'green', ls=':')\n",
    "        ax.axvline(min_all, label=f'All time series min: {min_all}', color = 'red', ls=':')\n",
    "        ax.set_ylim(50000, 500000)\n",
    "    \n",
    "        fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_ts_zips(df_sacramento_city_zips, sacto_zips_no95815)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Sacto Metro zipcodes by city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Here's the code for graphing over the dictionary\n",
    "# only plots one figure per row, so much longer to scroll through that iterative plotting functions\n",
    "# Standard y-axis across all plots \n",
    "\n",
    "for key in sorted(dict_sac_zips_cities.keys()):\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    plt.title(f\"{key}\")\n",
    "    for val in dict_sac_zips_cities[key]:\n",
    "        df_sac['value'].loc[df_sac['Zip'] == val].plot(ax = ax)\n",
    "        ax.set_ylim(50000, 1100000)\n",
    "        ax.legend(dict_sac_zips_cities[key], ncol=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# only plots one figure per row, so much longer to scroll through that iterative plotting functions\n",
    "# y-axis not standardized across all plots\n",
    "\n",
    "for key in sorted(dict_sac_zips_cities.keys()):\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    plt.title(f\"{key}\")\n",
    "    for val in dict_sac_zips_cities[key]:\n",
    "        df_sac['value'].loc[df_sac['Zip'] == val].plot(ax = ax)\n",
    "        ax.legend(dict_sac_zips_cities[key], ncol=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from James Irving's study group:\n",
    "    \n",
    "def plot_ts_zips(df, zipcodes, col='value'):\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 30))\n",
    "    \n",
    "    for i, zc in enumerate(zipcodes, start=1):\n",
    "        ax = fig.add_subplot(7, 3, i)\n",
    "        \n",
    "        ts = df[col].loc[df['Zip'] == zc]\n",
    "        ts.plot(title = zc, ax=ax)\n",
    "\n",
    "        max_ = ts.loc['2004':'2011'].idxmax()  \n",
    "        crash = '01-2009'\n",
    "        min_ = ts.loc[crash:].idxmin()\n",
    "\n",
    "        max_all = ts.idxmax()\n",
    "        min_all = ts.idxmin()\n",
    "\n",
    "        ax.axvline(max_, label=f'Max Price: {max_}', color = 'orange', ls=':')\n",
    "        ax.axvline(crash, label = 'Housing Index Drops', color='black')\n",
    "        ax.axvline(min_, label=f'Min Price Post Crash: {min_}', color = 'red', ls=':')\n",
    "        ax.axvline(max_all, label=f'All time series max {max_all}', color = 'green', ls=':')\n",
    "        ax.axvline(min_all, label=f'All time series min: {min_all}', color = 'red', ls=':')\n",
    "        ax.set_ylim(50000, 500000)\n",
    "    \n",
    "        fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_ts_zips(df_sacramento_city_zips, sacto_zips_no95815)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Box plots--code below only plots one figure per row, per city, so it's a long scroll-through \n",
    "\n",
    "# df_sac.drop('SizeRank', axis=1, inplace=True)\n",
    "\n",
    "# for key in sorted(dict_sac_zips_cities.keys()):\n",
    "#     fig, ax = plt.subplots(figsize=(12, 4))\n",
    "#     plt.title(f\"{key}\")\n",
    "#     for val in dict_sac_zips_cities[key]:\n",
    "#         df_sac.loc[df_sac['Zip'] == val].boxplot(ax = ax)\n",
    "#         ax.legend(dict_sac_zips_cities[key], ncol=3)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Creating df_sac_mo_city (monthly values by *City*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_mo_city = df_sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_mo_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_mo_city = df_sac.groupby(['CountyName', 'City', 'time']).mean()\n",
    "df_sac_mo_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_mo_city.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_mo_city.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_sac_mo_city.loc['1996-04-01':'2000-12-31']\n",
    "\n",
    "df_sac_mo_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Creating dataframe that shows max, min, dates for each, and % difference max-min / min for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_cities_max = {}\n",
    "dict_max_dates = {}\n",
    "max_values_cities = []\n",
    "max_values_dates = []\n",
    "\n",
    "for city in sac_metro_cities:\n",
    "    df_city = df_sac_mo_city.loc[df_sac_mo_city['City'] == city]\n",
    "    max_val = df_city.max()\n",
    "    max_values_cities.append(round(max_val['value'], 0))\n",
    "    max_date = df_city['value'].idxmax()\n",
    "    max_values_dates.append(max_date)\n",
    "\n",
    "dict_cities_max = dict(zip(sac_metro_cities, max_values_cities))\n",
    "dict_max_dates = dict(zip(sac_metro_cities, max_values_dates))\n",
    "dict_max_dates\n",
    "\n",
    "# max_ = ts.loc['2004':'2010'].idxmax()  # 625600\n",
    "# crash = '01-2009'\n",
    "# min_ = ts.loc[crash:].idxmin()\n",
    "\n",
    "# df_sac_mo_city.loc[df_sac_mo_city['City'] == 'Davis'].max()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs = pd.DataFrame.from_dict(dict_cities_max, orient='index', columns=['Max Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs.index.name = 'City'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "max_dates = pd.Series(dict_max_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs['Max Date'] = max_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dict_cities_min = {}\n",
    "dict_min_dates = {}\n",
    "min_values_cities = []\n",
    "min_values_dates = []\n",
    "\n",
    "for city in sac_metro_cities:\n",
    "    df_city = df_sac_mo_city.loc[df_sac_mo_city['City'] == city]\n",
    "    min_val = df_city.min()\n",
    "    min_values_cities.append(round(min_val['value'], 0))\n",
    "    min_date = df_city['value'].idxmin()\n",
    "    min_values_dates.append(min_date)\n",
    "\n",
    "dict_cities_min = dict(zip(sac_metro_cities, min_values_cities))\n",
    "dict_min_dates = dict(zip(sac_metro_cities, min_values_dates))\n",
    "# print(dict_max_dates)\n",
    "\n",
    "# print(dict_cities_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cities_min = pd.Series(dict_cities_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cities_min.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs['Min Value'] = cities_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "min_dates = pd.Series(dict_min_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs['Min Date'] = min_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs['pct_change'] = round(((df_sac_cities_calcs['Max Value'] - df_sac_cities_calcs['Min Value']) / df_sac_cities_calcs['Min Value']) * 100, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### df_sac_cities_calcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sac_cities_calcs.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Adapted from James Irving's study group:\n",
    "    \n",
    "def plot_ts_cities(df, cities, col='value'):\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 100))\n",
    "    \n",
    "    for i, city in enumerate(cities, start=1):\n",
    "        ax = fig.add_subplot(30,3,i)\n",
    "        \n",
    "        ts = df[col].loc[df['City'] == city]\n",
    "        ts.plot(title = str(city), ax=ax)\n",
    "\n",
    "        max_ = ts.loc['2004':'2011'].idxmax()  \n",
    "        crash = '01-2009'\n",
    "        min_ = ts.loc[crash:].idxmin()\n",
    "\n",
    "        max_all = ts.idxmax()\n",
    "        min_all = ts.idxmin()\n",
    "\n",
    "        ax.axvline(max_, label=f'Max Price: {max_}', color = 'orange', ls=':')\n",
    "        ax.axvline(crash, label = 'Housing Index Drops', color='black')\n",
    "        ax.axvline(min_, label=f'Min Price Post Crash: {min_}', color = 'red', ls=':')\n",
    "        ax.axvline(max_all, label=f'All time series max {max_all}', color = 'green', ls=':')\n",
    "        ax.axvline(min_all, label=f'All time series min: {min_all}', color = 'red', ls=':')\n",
    "\n",
    "        fig.tight_layout()\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_ts_cities(df_sac_mo_city, sac_metro_cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_davis = df_sac.loc[df_sac['City'] == 'Davis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_davis.sort_values(by=['Zip', 'time'], inplace=True)\n",
    "df_davis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for zipcode in df_davis['Zip']:     # zipcodes for Davis\n",
    "#     print(zipcode, df_davis['value'])\n",
    "\n",
    "for zipcode in df_davis['Zip']:\n",
    "    df_davis['value'].loc[df_davis['Zip'] == zipcode].plot(figsize=(12, 7))\n",
    "\n",
    "# Seasonality appears not to be a big factor in this city in the Sacramento Valley of California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_davis_201415 = df_davis.loc['2004':'2015']\n",
    "df_davis_201415.head(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for zipcode in df_davis_201415['Zip']:     # zipcodes for Woodland\n",
    "    df_davis_201415['value'].loc[df_davis_201415['Zip'] == zipcode].plot(figsize=(12, 7))\n",
    "\n",
    "    \n",
    "# very little seasonality is evident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting values by iterating through cities list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 12))\n",
    "\n",
    "for i in sac_metro_cities:\n",
    "    ax = df_sac_mo_city['value'].loc[df_sac_mo_city['City'] == i].plot(label = i)\n",
    "#     print(df_sac_yr['value'].loc[df_sac_yr['City'] == i])\n",
    "ax.legend(ncol=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting boxplots for each city in Sacto Metro region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(40,40))\n",
    "\n",
    "fig = plt.figure(figsize=(18, 40))\n",
    "\n",
    "for i, city in enumerate(sac_metro_cities, start=1):\n",
    "    ax = fig.add_subplot(10,6,i)\n",
    "    df_sac_mo_city.loc[df_sac_mo_city['City'] == city].boxplot(ax = ax)\n",
    "    ax.set_title(f'{city}')\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting boxplots for each zip in Sac Metro, by city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 30))\n",
    "\n",
    "for i, zc in enumerate(sac_metro_zips[:48], start=1):\n",
    "    ax = fig.add_subplot(8,6,i)\n",
    "    ts = df_sac.loc[df_sac['Zip'] == zc]\n",
    "    ts.boxplot(column = 'value', ax = ax)\n",
    "    ax.set_title(f'{zc}')\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dict_sac_zips_cities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first20pairs = {k: dict_sac_zips_cities[k] for k in list(dict_sac_zips_cities)[:20]}\n",
    "type(first20pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 50))\n",
    "\n",
    "for i, (city, zc) in enumerate(dict_sac_zips_cities.copy().items(), start=1):\n",
    "    ax = fig.add_subplot(14,6,i)\n",
    "    ax.set_title(f'{city}')\n",
    "    for i, zc in enumerate(dict_sac_zips_cities.copy()[city], start=1):\n",
    "        ts = df_sac.loc[df_sac['Zip'] == zc]\n",
    "        ts.boxplot(column = 'value')\n",
    "        fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sac_city = df_sac.loc[df_sac['City'] == 'Sacramento']\n",
    "\n",
    "df_sac_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sacto_zips = list(set(df_sac_city['Zip']))\n",
    "len(sacto_zips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sac_city.boxplot(by='Zip', column = 'value', figsize=(16, 6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 20))\n",
    "\n",
    "for i, zc in enumerate(sacto_zips, start=1):\n",
    "    ax = fig.add_subplot(5,5,i)\n",
    "    ax.set_title(f'{zc}')\n",
    "    ts = df_sac_city.loc[df_sac_city['Zip'] == zc]\n",
    "    ts.boxplot(by = 'Zip', column = 'value', ax=ax)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Violin plot for Sacramento city zip codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# got figure size  modification example from https://exceptionshub.com/how-do-i-change-the-figure-size-for-a-seaborn-plot.html\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# fig.set_size_inches(16, 6)\n",
    "# sns.violinplot(x=\"Zip\", y=\"value\", data=df_44, scale=\"count\", inner=\"stick\", ax=ax)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 6)\n",
    "sns.violinplot(x=\"Zip\", y=\"value\", data=df_sac_city, scale=\"width\", inner=\"quartile\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sac_city_1996_1999 = df_sac_city.loc['1996-04-01':'1999-12-01']\n",
    "df_sac_city_1996_1999.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 6)\n",
    "sns.violinplot(x=\"Zip\", y=\"value\", data=df_sac_city_1996_1999, scale=\"width\", inner=\"quartile\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sac_city_2000_2003 = df_sac_city.loc['2000-01-01':'2003-12-01']\n",
    "df_sac_city_2000_2003.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 6)\n",
    "sns.violinplot(x=\"Zip\", y=\"value\", data=df_sac_city_2000_2003, scale=\"width\", inner=\"quartile\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sac_city_2004_2006 = df_sac_city.loc['2004-01-01':'2006-12-01']\n",
    "df_sac_city_2004_2006.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 6)\n",
    "sns.violinplot(x=\"Zip\", y=\"value\", data=df_sac_city_2004_2006, scale=\"width\", inner=\"quartile\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sac_city_2007_2012 = df_sac_city.loc['2007-01-01':'2012-12-01']\n",
    "df_sac_city_2007_2012.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 6)\n",
    "sns.violinplot(x=\"Zip\", y=\"value\", data=df_sac_city_2007_2012, scale=\"width\", inner=\"quartile\", ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sac_city_2013_2018 = df_sac_city.loc['2013-01-01':'2018']\n",
    "df_sac_city_2013_2018.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(16, 6)\n",
    "sns.violinplot(x=\"Zip\", y=\"value\", data=df_sac_city_2013_2018, scale=\"width\", inner=\"quartile\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Line plot for Davis, CA, marking height of market bubble, drop of housing index on national level, and min for Davis post crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Looking at a single zip code (mine)\n",
    "\n",
    "ts = df_sac['value'].loc[df_sac['Zip'] == '95616']\n",
    "ax = ts.plot(figsize=(10, 6), label = 'Raw Price')\n",
    "\n",
    "max_ = ts.loc['2004':'2010'].idxmax()  # 625600\n",
    "crash = '01-2009'\n",
    "min_ = ts.loc[crash:].idxmin()\n",
    "\n",
    "\n",
    "ax.axvline(max_, label='Max Price', color = 'green', ls=':')\n",
    "ax.axvline(crash, label = 'Housing Index Drops', color='red', ls=':')\n",
    "ax.axvline(min_, label=f'Min Price Post Crash {min_}', color = 'black')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Step 3: EDA and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Framework from James Irving's study group for creating a plotting function that allows\n",
    "# entry of zip codes of interest\n",
    "\n",
    "def plot_ts(df_melt, col='value', zipcodes=['95616']):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for zc in zipcodes:\n",
    "        ts = df_melt[col].loc[df_melt['Zip'] == zc]\n",
    "        ts.plot(figsize=(12, 8), label = str(zc), ax=ax)\n",
    "\n",
    "    max_ = ts.loc['2004':'2010'].idxmax()  # 625600\n",
    "    crash = '01-2009'\n",
    "    min_ = ts.loc[crash:].idxmin()\n",
    "\n",
    "    ax.axvline(max_, label='Max Price', color = 'green', ls=':')\n",
    "    ax.axvline(crash, label = 'Housing Index Drops', color='red', ls=':')\n",
    "    ax.axvline(min_, label=f'Min Price Post Crash {min_}', color = 'black')\n",
    "    ax.legend()\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_ts(df_melt, zipcodes=['95616', '92008', '90035', '90025', '60625', '57701'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Framework from James Irving's study group\n",
    "\n",
    "def plot_ts(df_melt, col='value', zipcodes=['95616']):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for zc in zipcodes:\n",
    "        ts = df_melt[col].loc[df_melt['Zip'] == zc]\n",
    "        ts.plot(figsize=(12, 8), label = str(zc), ax=ax)\n",
    "\n",
    "    max_ = ts.loc['2004':'2010'].idxmax()  # 625600\n",
    "    crash = '01-2009'\n",
    "    min_ = ts.loc[crash:].idxmin()\n",
    "\n",
    "    ax.axvline(max_, label='Max Price', color = 'green', ls=':')\n",
    "    ax.axvline(crash, label = 'Housing Index Drops', color='red', ls=':')\n",
    "    ax.axvline(min_, label=f'Min Price Post Crash {min_}', color = 'black')\n",
    "    ax.legend()\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plot_ts(df_melt, zipcodes=['95616', '92008', '90035', '90025', '60625', '57701'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dict_metrostate_zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sub = df_melt.loc['2014-01-01':].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sub.sort_values(by=['Zip', 'time']).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Set ts = zip code for initial test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Example:  57701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts = df_melt.loc[df_melt['Zip'] == '95616']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ts = ts.resample('MS').asfreq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def plot_ts(df_melt, col='value', zipcodes=['95616']):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for zc in zipcodes:\n",
    "        ts = df_melt[col].loc[df_melt['Zip'] == zc]\n",
    "        ts.plot(figsize=(12, 8), label = str(zc), ax=ax)\n",
    "\n",
    "    max_ = ts.loc['2004':'2010'].idxmax()  # 625600\n",
    "    crash = '01-2009'\n",
    "    min_ = ts.loc[crash:].idxmin()\n",
    "    max_all = ts.idxmax()\n",
    "    min_all = ts.idxmin()\n",
    "    mean_all = ts.mean()\n",
    "\n",
    "    ax.axvline('2004-01-01', label=f'2004', color = 'black')\n",
    "    ax.axvline('2011-12-01', label=f'2010', color = 'black')\n",
    "    ax.axvline(max_, label='Max Price in 2004 - 2010 timeframe', color = 'green', ls=':')\n",
    "    ax.axvline(crash, label = 'Housing Index Drops', color='black', ls=':')\n",
    "    ax.axvline(min_, label=f'Min Price Post Crash (2004 - 2010 timeframe) {min_}', color = 'red', ls=':')\n",
    "    ax.axvline(max_all, label='All time series max', color = 'green', ls=':')\n",
    "    ax.axvline(mean_all, label = 'All time series mean', color='red', ls=':')\n",
    "    ax.axvline(min_all, label=f'All time series min: {min_}', color = 'red', ls=':')\n",
    "\n",
    "    ax.legend()\n",
    "    \n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_ts(df_melt, col='value', zipcodes=['95616'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Creating ACF and PACF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from pandas.plotting import autocorrelation_plot, lag_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_acf_pacf(ts, figsize=(10,6), lags=15):\n",
    "    fig, ax = plt.subplots(nrows=2, figsize=figsize)\n",
    "    plot_acf(ts, ax=ax[0], lags=lags)\n",
    "    plot_pacf(ts, ax=ax[1], lags=lags)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    for a in ax:\n",
    "        a.xaxis.set_major_locator(mpl.ticker.MaxNLocator(min_n_ticks=lags, integer=True))\n",
    "        a.xaxis.grid()\n",
    "    \n",
    "plot_acf_pacf(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from James Irving's study group\n",
    "# plot seasonal decomposition\n",
    "\n",
    "def plot_seasonal_decomp(ts):\n",
    "    decomp = seasonal_decompose(ts)\n",
    "    ts_seasonal = decomp.seasonal\n",
    "\n",
    "    ax = ts_seasonal.plot()\n",
    "    fig = ax.get_figure()\n",
    "    fig.set_size_inches(18,6)\n",
    "\n",
    "    min_ = ts_seasonal.idxmin()\n",
    "    max_ = ts_seasonal.idxmax()\n",
    "    max_2 = ts_seasonal.loc[min_:].idxmax()\n",
    "    min_2 = ts_seasonal.loc[max_2:].idxmin()\n",
    "\n",
    "\n",
    "    ax.axvline(min_, label=min_, c='orange')\n",
    "    ax.axvline(max_, c='orange', ls=':')\n",
    "    ax.axvline(min_2, c='orange')\n",
    "    ax.axvline(max_2, c='orange', ls=':')\n",
    "\n",
    "    period = min_2 - min_ \n",
    "    ax.set_title(f'Season Length = {period}')\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Seasonality isn't much of a factor... maybe 0.1% in recent years from the lowest point in the year to the highest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_seasonal_decomp(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4:  ARIMA Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.head()  # check ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating p, d, q, and m values for running ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "p_range = [0, 1, 2, 4, 8, 10]\n",
    "q_range = range(0, 3)\n",
    "d_range = range(1, 3)\n",
    "m_range = (0, 6, 12)\n",
    "\n",
    "pdq = list(itertools.product(p_range, d_range, q_range))\n",
    "print(pdq)\n",
    "print()\n",
    "PDQM = list(itertools.product(p_range, d_range, q_range, m_range))\n",
    "print(PDQM[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_pdq_pdqm(p_range=(0,4), d_range=(0,3), q_range=(0,4), make_seasonal=True,\n",
    "                  m_values=(0,12)):\n",
    "    import itertools\n",
    "    p_values =range(p_range[0],p_range[1])\n",
    "    d_values =range(d_range[0],d_range[1])\n",
    "    q_values =range(q_range[0],q_range[1])\n",
    "    \n",
    "    params = {}\n",
    "    params['pdq'] = list(itertools.product(p_values, d_values, q_values))\n",
    "    \n",
    "    if make_seasonal:\n",
    "        params['PDQm'] = list(itertools.product(p_values, d_values, q_values, m_values))\n",
    "    return params\n",
    "\n",
    "params = make_pdq_pdqm()\n",
    "print(params.keys())\n",
    "print(params['pdq'][:20])\n",
    "params['PDQm'][:20]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up functions for running ARIMA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = df_melt['value'].loc[df_melt['Zip'] == '95616']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_predict_error(X, arima_order):\n",
    "    train_size = int(len(X) * .85)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    predictions = list()\n",
    "    history = [x for x in train]\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order = arima_order)\n",
    "        model_fit = model.fit(disp=0)\n",
    "        y_hat = model_fit.forecast()[0]\n",
    "        predictions.append(y_hat)\n",
    "        history.append(test[t])\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    return error\n",
    "\n",
    "def eval_arima_models(data, p_values, d_values, q_values):\n",
    "    data = data.astype('float32')\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    mse = arima_predict_error(data, order)\n",
    "                    if mse < best_score:\n",
    "                        best_score, best_cfg = mse, order\n",
    "                    print('ARIMA%s MSE=%.3f' % (order,mse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate parameters\n",
    "p_values = [0, 1, 2, 4, 8]\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 3)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# eval_arima_models(ts.values, p_values, d_values, q_values)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Interpreting Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ARIMA model and show summary results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "\n",
    "def arima_zipcode(ts, order=None):\n",
    "    model = ARIMA(ts.values, order = (1,1,1))\n",
    "    model_fit = model.fit(disp=0)\n",
    "    print(model_fit.summary())\n",
    "    return model_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_fit = arima_zipcode(ts, order = (1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create forecast model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "\n",
    "def forecast(model_fit, months=24, confint=2):\n",
    "    forecast = model_fit.forecast(months)\n",
    "    actual_forecast = forecast[0]\n",
    "    forecast_confint = forecast[confint]\n",
    "    return actual_forecast, forecast_confint   # note that this leaves behind the middle range of numbers in the forecast result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_forecast, forecast_confint = forecast(model_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe to hold these values and join to existing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "\n",
    "def forecast_df(col = 'time', daterange = pd.date_range(start='2018-05-01', end='2020-04-01', freq='MS'), \n",
    "                actual_forecast=actual_forecast, forecast_confint = forecast_confint):\n",
    "    df_forecast = pd.DataFrame({col: daterange})\n",
    "    df_forecast['forecast'] = actual_forecast\n",
    "    df_forecast['forecast_lower'] = forecast_confint[:, 0]\n",
    "    df_forecast['forecast_upper'] = forecast_confint[:, 1]\n",
    "    df_forecast.set_index('time', inplace=True)\n",
    "    return df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_forecast = forecast_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create df_new with historical and forecasted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_values_forecast(ts, df_forecast):\n",
    "    df_new = pd.concat([ts, df_forecast])\n",
    "    df_new = df_new.rename(columns = {0: 'value'})\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = concat_values_forecast(ts, df_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot forecast results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "\n",
    "def plot_forecast(df = df_new, figsize=(12,8)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.plot(df_new['value'], label='Raw Data')\n",
    "    plt.plot(df_new['forecast'], label='Forecast')\n",
    "    plt.fill_between(df_new.index, df_new['forecast_lower'], df_new['forecast_upper'], color='k', alpha = 0.2, \n",
    "                 label='Confidence Interval')\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.title('Forecast for 57701')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out percent change in home values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "\n",
    "def forecast_values(df = df_new, date = '2020-04-01'):\n",
    "    forecasted_price = df.loc['2020-04-01', 'forecast']\n",
    "    forecasted_lower = df.loc['2020-04-01', 'forecast_lower']\n",
    "    forecasted_upper = df.loc['2020-04-01', 'forecast_upper']    \n",
    "    return forecasted_price, forecasted_lower, forecasted_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted_price, forecasted_lower, forecasted_upper = forecast_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_value(df = df_new, date = '2018-04-01'):\n",
    "    last_value = df.loc[date, 'value']\n",
    "    return last_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_value = last_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and print predicted, best, and worst case scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "\n",
    "def pred_best_worst(pred, low, high, last, date='April 1, 2020'):\n",
    "    pred_pct_change = (((pred - last) / last) * 100)\n",
    "    print(f'By the model prediction, I would expect to see a {round(pred_pct_change, 3)}% change in price by April 1, 2020')\n",
    "    lower_pct_change = ((low - last) / last) * 100\n",
    "    print(f'At the lower bound of the confidence interval, I would expect to see a {round(lower_pct_change, 3)}% change in price by April 1, 2020')\n",
    "    upper_pct_change = ((high - last) / last) * 100\n",
    "    print(f'At the upper bound of the confidence interval, I would expect to see a {round(upper_pct_change, 3)}% change in price by April 1, 2020')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_value = last_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_best_worst(pred=forecasted_price, low=forecasted_lower, high=forecasted_upper, last=last_value, date='April 1, 2020')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUICK RUN:  List of functions to run for each zip code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot zip code(s) of interest over time, with market highs and lows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_ts(df_melt, zipcodes=['95616', '92008', '90035', '90025', '60625', '57701'])  # enter desired zip code(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Create ts dataframe for *single* zip code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Enter zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "zipcode = '95616'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Create dataframe from df_melt containing values for just this zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ts = df_melt['value'].loc[df_melt['Zip'] == zipcode]  # ts for all of the following functions should have only one zipcode\n",
    "ts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ACF and PACF for ts dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_acf_pacf(ts, figsize=(10,6), lags=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot seasonal decomposition for ts dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_seasonal_decomp(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate list of (p,d,q) and (p,d,q,m) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = make_pdq_pdqm(p_range=(0,4), d_range=(0,3), q_range=(0,4), make_seasonal=True,\n",
    "                  m_values=(0,12))\n",
    "print(params['pdq'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ARIMA model on ts dataframe with list of p,d,q values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_arima_models(data, p_values, d_values, q_values)  # don't run this until you have some time, since it take time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General model fit summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = arima_zipcode(ts, order = (2,1,2))   # change order = tuple to correspond to the best MSE produced by previous function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce forecasts from ARIMA model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_forecast, forecast_confint = forecast(model_fit, months=24, confint=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe to hold forecast results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast = forecast_df(col = 'time', daterange = pd.date_range(start='2018-05-01', end='2020-04-01', freq='MS'), \n",
    "                actual_forecast=actual_forecast, forecast_confint = forecast_confint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new dataframe concatenating historic values and forecast results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = concat_values_forecast(ts, df_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate predicted, lower bound, and upper bound forecasted prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasted_price, forecasted_lower, forecasted_upper = forecast_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_value = last_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot forecast with confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast(df = df_new, figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print summary of predicted, worst-case, and best-case scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_best_worst(pred=forecasted_price, low=forecasted_lower, high=forecasted_upper, last=last_value, date='April 1, 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Ancillary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Setting time and Zip as indices (df_melt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# From James Irving's study group (one way to create a dataframe)\n",
    "\n",
    "# df_melt2 = df_melt.groupby('Zip').resample('MS').asfreq()\n",
    "# df_melt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Creating df_cal from df_melt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_cal = df_melt2.loc[df_melt2.State == 'CA']\n",
    "# df_cal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df_cal.Metro.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# met_list = []\n",
    "# cty_list = []\n",
    "\n",
    "# for i in df_cal.Metro:\n",
    "#     for j in df_cal.CountyName:\n",
    "#         met_list.append(i)\n",
    "#         cty_list.append(j)\n",
    "        \n",
    "# print(met_list)\n",
    "# print(cty_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# met_list_unique = set(met_list)\n",
    "# cty_list_unique = set(cty_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set(df_cal.Metro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# set(df_cal.CountyName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# # Counter(df_cal.Metro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Counter(df_cal.CountyName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pd.get_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_sac.loc[df_sac['City']=='Sacramento']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## List of functions to run for each zip code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# functions \n",
    "\n",
    "fig, ax = plot_ts(df_melt, zipcodes=['95616', '92008', '90035', '90025', '60625', '57701'])  # enter desired zip code(s)\n",
    "# ts = df_melt.loc[df_melt['Zip'] == '95616']\n",
    "plot_acf_pacf(ts, figsize=(10,6), lags=15)\n",
    "plot_seasonal_decomp(ts)\n",
    "make_pdq_pdqm(p_range=(0,4), d_range=(0,3), q_range=(0,4), make_seasonal=True,\n",
    "                  m_values=(0,12))\n",
    "params = make_pdq_pdqm()\n",
    "# print(params.keys())                     # uncomment to print / show results\n",
    "# print(params['pdq'][:20])                # uncomment to print / show results\n",
    "# params['PDQm'][:20]                      # uncomment to print / show results\n",
    "\n",
    "# arima_predict_error(X, arima_order)      # This function feeds into the next one.  You need to be sure that the function  \n",
    "                                           # definition has been run, so that the kernel knows what it is, before \n",
    "                                           # running the next one, but you don't need to call this function before\n",
    "                                           # calling the next one.  The next function calls this one and automatically\n",
    "                                           # populates it with parameters.\n",
    "\n",
    "eval_arima_models(data, p_values, d_values, q_values)\n",
    "model_fit = arima_zipcode(ts, order = (1,1,1))\n",
    "actual_forecast, forecast_confint = forecast(model_fit, months=24, confint=2)\n",
    "df_forecast = forecast_df(col = 'time', daterange = pd.date_range(start='2018-05-01', end='2020-04-01', freq='MS'), \n",
    "                actual_forecast=actual_forecast, forecast_confint = forecast_confint)\n",
    "df_new = concat_values_forecast(ts, df_forecast)\n",
    "forecasted_price, forecasted_lower, forecasted_upper = forecast_values()   # keep these variable names to make printout easier\n",
    "last_value = last_value(df = df_new, date = '2018-04-01')      # keep these variable names to make printout function below easier\n",
    "plot_forecast(df = df_new, figsize=(12,8))\n",
    "pred_best_worst(pred=forecasted_price, low=forecasted_lower, high=forecasted_upper, last=last_value, date='April 1, 2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # evaluate an ARIMA model for a given order (p,d,q)\n",
    "# def evaluate_arima_model(X, arima_order):\n",
    "#     # prepare training dataset\n",
    "#     train_size = int(len(X) * 0.85)\n",
    "#     train, test = X[0:train_size], X[train_size:]\n",
    "#     history = [x for x in train]\n",
    "#     # make predictions\n",
    "#     predictions = list()\n",
    "#     for t in range(len(test)):\n",
    "#         model = ARIMA(history, order=arima_order)\n",
    "#         model_fit = model.fit(disp=0)\n",
    "#         yhat = model_fit.forecast()[0]\n",
    "#         predictions.append(yhat)\n",
    "#         history.append(test[t])\n",
    "#     # calculate out of sample error\n",
    "#     error = mean_squared_error(test, predictions)\n",
    "#     return error\n",
    "\n",
    "# # evaluate combinations of p, d and q values for an ARIMA model\n",
    "# def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "#     dataset = dataset.astype('float32')\n",
    "#     best_score, best_cfg = float(\"inf\"), None\n",
    "#     for p in p_values:\n",
    "#         for d in d_values:\n",
    "#             for q in q_values:\n",
    "#                 order = (p,d,q)\n",
    "#                 try:\n",
    "#                     mse = evaluate_arima_model(dataset, order)\n",
    "#                     if mse < best_score:\n",
    "#                         best_score, best_cfg = mse, order\n",
    "#                     print('ARIMA%s MSE=%.3f' % (order,mse))\n",
    "#                 except:\n",
    "#                     continue\n",
    "#     print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Obtain train-test-split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_train_test_split_index(ts, TEST_SIZE=0.1,min_test_ts=2):\n",
    "#     import math\n",
    "#     idx_split = math.floor(len(ts.index)*(1-TEST_SIZE))\n",
    "    \n",
    "#     num_test_ts=len(ts.iloc[idx_split:])\n",
    "#     if num_test_ts<min_test_ts:\n",
    "#         print(f'[!] Warning: using TEST_SIZE={TEST_SIZE} produced {num_test_ts} test timestamps.')\n",
    "#         print(\"- Overriding TEST_SIZE and using min_test_ts instead.\")\n",
    "#         idx_split = len(ts)-min_test_ts\n",
    "    \n",
    "#     return idx_split\n",
    "\n",
    "# def train_test_split_ts(ts, TEST_SIZE = 0.2, min_test_ts = 2):\n",
    "#     idx_split = get_train_test_split_index(ts, TEST_SIZE = TEST_SIZE, min_test_ts = min_test_ts)\n",
    "#     ts_train = ts.iloc[:idx_split]\n",
    "#     ts_test = ts.iloc[idx_split:]\n",
    "#     return ts_train, ts_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Define \"get now\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_now(return_dt = True, return_str=True, show=False):\n",
    "#     import datetime as dt\n",
    "#     import tzlocal as tz\n",
    "#     now=dt.datetime.now(tz=tz.get_localzone())\n",
    "#     str_time =now.strftime('%m/%d/%Y - %I:%M:%S %p')\n",
    "#     if show:\n",
    "#         print(str_time)\n",
    "    \n",
    "#     output=[]\n",
    "#     if return_dt:\n",
    "#         output.append(now)\n",
    "        \n",
    "#     if return_str:\n",
    "#         output.append(str_time)\n",
    "#     return output[:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Import and run SARIMAX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "# def grid_search_arimax(ts_train,pdq=None,pdqm=None,order_dict=None,\n",
    "#                         verbose=False, model_kws={}):\n",
    "\n",
    "#     from tqdm import trange\n",
    "    \n",
    "#     if (pdq is None) & (order_dict is not None):\n",
    "#         pdq= order_dict['pdq']\n",
    "#     if (pdqm is None) & (order_dict is not None):\n",
    "#         pdqm= order_dict['PDQm']\n",
    "\n",
    "# #     import tqdm\n",
    "# #     from tqdm import trange\n",
    "\n",
    "#     start,start_str = get_now(return_dt=True, return_str=True)\n",
    "#     print(f'[i] STARTING GRID SEARCH @ {start_str}:')\n",
    "\n",
    "#     res = [['pdq','PDQM','AIC']]\n",
    "#     for i in trange((len(pdq))):\n",
    "#         comb = pdq[i]\n",
    "        \n",
    "#         for combs in pdqm:\n",
    "#             try: \n",
    "#                 model = SARIMAX(ts_train, order=comb, \n",
    "#                                seasonal_orde =combs,enforce_stationarity=False,\n",
    "#                                enforce_invertibility=False,**model_kws)\n",
    "                \n",
    "#                 output= model.fit()\n",
    "#                 res.append([comb,combs,output.aic])\n",
    "#             except:\n",
    "#                 if verbose:\n",
    "#                     print(f\"[!] Error running ({comb})({combs})\")\n",
    "#                 continue\n",
    "#     end,end_str = get_now(return_dt=True, return_str=True)\n",
    "#     elapsed = end-start\n",
    "#     print(f\"[i] GRID SEARCH FINISHED AT {end_str}\")\n",
    "#     print(f\"\\tTotal Time: {elapsed}\")\n",
    "\n",
    "#     try:\n",
    "#         df_res =fs.list2df(res)\n",
    "#         return df_res\n",
    "    \n",
    "#     except:\n",
    "#         print('Error converting to df. Returning as list.')\n",
    "#         return res \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# res = grid_search_sarimax(ts_train, order_dict=params) #pdq, pdqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Project Notebook Settings\n",
    "# pd.set_option('display.max_columns',0)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# plt.style.use('seaborn-notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "540px",
    "left": "78px",
    "top": "109.8px",
    "width": "281px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
